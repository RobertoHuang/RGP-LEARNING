# 可靠性探究

## 副本剖析

> 副本是分布式系统中常见的概念之一，指的的分布式系统对数据和服务提供的一种冗余的方式
>
> 数据副本是指在不同的节点上持久化同一份数据，当某个节点上存储的数据丢失时可以从副本上读取该数据
>
> 服务副本是指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理

### 失效副本

`Kafka`副本管理器会启动一个副本过期检测的定时任务，定时检查当前时间与副本的`lastCaughtUpTimeMs`(最后赶上时间)差值是否大于参数`replica.lag.time.max.ms`指定的值。`lastCaughtUpTimeMs`在以下两种情况会得到更新

- `fetchOffset>=leo`则将当前`lastCaughtUpTimeMs`设置为本次副本同步时间
- `fetchOffset >= this.lastFetchLeaderEndOffset`，即此次拉取请求比上一次拉取时`Leader`副本的`LEO`要大(该机制是为了防止`Leader`副本一直有数据写入，导致`lastCaughtUpTimeMs`无法及时得到更新)，则设置`lastCaughtUpTimeMs`为上一次拉取时间

`Kafka`源码注释中说明了一般有两种情况会导致副本失效

- `follower`副本进程卡住，在一段时间内根本没有向`Leader`副本发起同步请求，并入频繁`Full GC`
- `follower`副本进程同步过慢，在一段时间内都无法追赶上`Leader`副本，比如`I/O`开销过大

### ISR伸缩

**注:在`ISR`集合发生变化后，可能会引起`HW`值的改变**

- `isr-expiration`任务会周期性地检查每个分区是否需要缩减其`ISR`集合，当检测到`ISR`集合中有失效副本时就会收缩`ISR`。如果某个分区的`ISR`集合发生变更则会将变更后的数据记录到`Zookeeper`对应的节点中，并且将变化的记录缓存到`isrChangeSet`中
- 随着`follower`副本不断与`leader`副本进行消息同步，`follower`副本的`LEO`会逐渐后移并最终追上`Leader`副本，此时该`follower`副本就有资格进入`ISR`集合。追赶上`leader`副本的判定准则是此副本的`LEO`是否不小于`leader`副本的`HW`，注意这里并不是和`leader`副本的`LEO`相比。`ISR`扩充之后同样会更新`Zookeeper`对应节点并且将变化的记录缓存到`isrChangeSet`中

`isr-change-propagation`任务会周期性的检查`ISR`集合是否发生变化(即检查`isrChangeSet`中是否有元素)，如果发现`isrChangeSet`中有`ISR`集合的变更记录，那么它会在`Zookeeper`的`/isr_change_notification`路径下创建一个以`isr_change_`开头的持久顺序节点，并将`isrChangeSet`信息保存到这个节点中

`Kafka`控制器为`/isr_change_notification`添加了一个`Watch`，当该节点有子节点发生变化时会触发`Watcher`的动作，以此通知控制器更新相关元数据信息并向它管理的`broker`节点发送更新元数据请求，最后清空路径下已处理过的节点。之所以要把`ISR`缩放通过两个线程来操作，我觉得该设计是希望利用缓存`ISR`变化减少`Watch`频繁触发。为了减少`Watch`频繁的被触发`Kafka`还提供了如下限定条件:

- 上一次`ISR`集合变化距离现在已超过5S
- 上一次写入`Zookeeper`的时间距离现在已经超过60s

满足如上两个条件之一才可以将`ISR`集合的变化写入目标节点

### LEO和HW

关于`LEO`和`HW`的概念定义在之前的博文中已介绍，在一个分区中`leader`副本所在的节点会记录所有副本`LEO`(用于计算`HW`)，而`follower`副本所在的节点都只记录它自身的`LEO`而不会记录其他副本的`LEO`，对`HW`而言各个副本所在的节点都只记录它自身的`HW`。在`Kafka`中分别有定时任务将`LEO`和`HW`刷写到恢复点文件

- `highwatermark-checkpoint`根据`broker`端参数`replica.high.watermark.checkpoint.interval.ms`配置的时间定时将所有分区的`HW`刷写到恢复点文件`replication-offset-checkpoint`中
- `kafka-recovery-point-checkpoint`根据`broker`端参数`log.flush.offset.checkpoint.interval.ms`配置的时间定时将所有分区的`LEO`刷写到恢复点文件`recovery-point-offset-checkpoint`中

### Leader Epoch

早期根据`Leader`副本`HW`的值做截断可能会存在数据丢失和数据不一致的情况，场景如下

- `Replica B`是当前的`leader`副本，`Replica A`是当前的`follower`副本。在某一时刻`B`中有两条消息`m1`和`m2`，`A`从`B`中同步了这两条消息此时`A`和`B`的`LEO`都为2同时`HW`都为1，之后`A`再向`B`中发送请求拉取消息，`FetchRequest`请求带上了`A`的`LEO`信息，`B`在收到请求之后更新自己的`HW`为2，`B`中虽然没有更多的消息但还是在延时一段时间后返回`FetchResponse`，并在其中包含了`HW`的信息，最后`A`根据`FetchResponse`中`HW`信息更新自己的`HW`为2

  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E5%9C%BA%E6%99%AFA.png"/></div>
可以看到整个过程中两者之间的`HW`同步有一个间隙，在`A`写入消息`m2`之后(`LEO`更新为2)需要再一轮的`Fetch`请求才能更新自身的`HW`为2，如果在这个时候`A`宕机了，那么`A`会根据之前`HW`的位置(这个值会存入本地的复制点文件`replication-offset-checkpoint`)进行日志截断，这样便会将`m2`这条消息删除，此时`A`只剩`m1`这一条消息，之后`A`再向`B`发送`FetchRequest`请求拉取消息
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E5%9C%BA%E6%99%AFB.png"/></div>
  此时若`B`再宕机那么`A`就会选举为新`Leader`，`B`恢复之后会成为`follower`，由于`follower`副本`HW`不能比`leader`副本的`HW`高，所以还会做一次日志截断以此将`HW`调整为1，这样一来`m2`这条消息就丢失了(就算B不能恢复，这条消息也同样丢失)
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E5%9C%BA%E6%99%AFC.png"/></div>
- 当前`Leader`副本为`A`，`follower`副本为`B`，`A`中有2条消息`m1`和`m2,`并且`HW`和`LEO`都为2，`B`中有一条消息`m1`并且`HW`和`LEO`都为1,。假设`A`、`B`同时挂掉然后`B`第一个恢复过来并成为`leader`如图所示

  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4A.png"/></div>
之后`B`写入消息`m3`，并将`LEO`和`HW`更新至2(假设所有场景中的`min.insync.replicas`参数配置为1)，此时`A`也恢复过来了，根据前面数据丢失场景中的介绍可知它会赋予`follower`的角色，并且需要根据`HW`截断日志及发送`FetchRequest`至`B`，不过此时`A`的`HW`真好也为2，那么就可以不做任何调整
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4B.png"/></div>
  如此一来`A`中保留了`m2`而`B`中没有，`B`中新增`m3`而`A`也同步不到，这样`A`和`B`就出现了数据不一致的情形
  
- `Leader Epoch`解决数据丢失问题

  `Replica B`是当前的`leader`副本，`Replica A`是当前的`follower`副本，并且当前`A`和`B`中的`LE`都为0

  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1A.png"/></div>

  
  同样`A`发生重启之后`A`不是先忙着截断日志而是先发送`OffsetsForLeaderEpochRequest`请求给`B`(其中包含当前的`LeaderEpoch`值)，`B`作为目前的`Leader`在收到请求之后会返回当前请求`LeaderEpoch`对应的`LEO`，如果`A`中的`LeaderEpoch`(假设为`LE_A`)和`B`中的不相同，那么`B`此时会查找`LeaderEpoch`为`LE_A+1`对应的`StartOffset`并返回给`A`，也就是`LE_A`对应的`LEO`，所以我们将`OffsetsForLeaderEpochRequest`的请求看作用来查找`follower`副本当前`LeaderEpoch`的`LEO`
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1B.png"/></div>
  
  
  `A`在收到`OffsetsForLeaderEpochResponse`之后发现和目前的`LEO`相同，也就不需要截断日志。之后`B`发生宕机`A`成为新`leader`，那么对应的`LE=0`也变成`LE=1`，对应的消息`m2`此时就得到了保留。之后不管`B`有没有恢复后续的消息都可以以`LE1`为`LeaderEpoch`陆续追加到`A`中
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1C.png"/></div>
  
  
- `Leader Epoch`解决数据不一致问题

  当前`A`为`leader`，`B`为`follower`，`A`中有两条消息`m1`和`m2`，而`B`中有一条消息`m1`。假设`A`和`B`同时挂掉然后`B`第一个恢复过来并成为新`leader`

  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4A.png"/></div>
之后`B`写入消息`m3`，并将`LEO`和`HW`更新至2，注意此时的`LeaderEpoch`已经从`LEO`增至`LE1`了
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4B.png"/></div>
  紧接着`A`也恢复过来成为`follower`并向`B`发送`OffsetsForLeaderEpochRequest`请求，此时`A`的`LeaderEpoch`为`LEO`。`B`根据`LEO`查询到对应的`offset`为1并返回给`A`，`A`就截断日志并删除了消息`m2`，之后`A`发送`FetchRequest`至`B`来同步数据，最终`A`和`B`中都有两条消息`m1`和`m3`，`HW`和`LEO`都为2，并且`LeaderEpoch`都为1，如此便解决了数据不一致问题
  
  <div align=center><img src="https://raw.githubusercontent.com/RobertoHuang/RGP-LEARNING/master/Kafka/images/%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4C.png"/></div>
### 为什么不支持读写分离

- 数据不一致问题
- 延时问题，从主节点同步到副本节点

- `Kafka`分区分配已经可以保证每个`broker`的读写负载都是一样的

## 可靠性分析

- 副本数量(副本数为3即可满足绝大多数场景需求，对可靠性要求更高的场景下可以适当增加这个数值，比如国内部分银行在使用`kafka`时就会设置副本数为5)，与此同时如果能够在分配分区副本时引入机架信息，那么还要应对机架整体宕机的风险
- `ACK`机制
  - `ACK=0`表示`producer`不等待`broker`的`ack`，`broker`一接受到数据还没有写入磁盘就已经返回
  - `ACK=1`表示生产者将消息发送到`leader`副本，`leader`副本在成功写入本地日志文件之后返回
  - `ACK=-1`表示生产者将消息发送到`leader`副本，`leader`副本在成功写入本地日志之后还要等待`ISR`中的`follower`副本全部同步完成才能够告知生产者已经成功提交，该方式最可靠但性能也最低
- 发送方式【发后即忘、同步和异步】，如果要提高可靠性那么生产者可以采用同步或异步模式，在出现异常情况时可以及时获得通知以便可以做相应的补救措施，比如选择重试发送等
- 有些发送异常属于可重试异常比如`NetworkException`，这个可能是由瞬间的网络故障而导致的，一般通过重试就可以解决。对于这种异常直接抛给客户端的适用方也未免过于兴师动众，客户端内部本身提供了重试机制来应对这种类型的异常，可以通过`retries`参数即可配置，默认情况下`retries`参数设置为0即不重试，对于高可靠性要求的场景需要将这个值设置为大于0，可以通过配置`retry.backoff.ms`设置两次重试之间的时间间隔，以此来避免无效的频繁重试。如果配置的`retries`参数值大于0则可能引起一些负面的影响，这样可能会影响消息的顺序性，可以通过设置`max.in.flight.requests.per.connection`为1，这样也就放弃了吞吐
- 如果`leader`副本的消息流入速度很快而`follower`副本的同步速度很慢，在某个临界点所有的`follower`都被踢出了`ISR`集合，那么`ISR`中就只有一个`leader`副本，最终`acks=-1`演变为`acks=1`的情形，如此也就加大了丢消息的风险。`Kafka`也考虑到了这种情况并提供`min.insync.replicas`参数来作为辅助，如果`ISR`集合小于指定的`ISR`集合中最小的副本数，则会使消息无法写入，抛出相应异常
- `unclean.leader.election.enable`，如果设置为`true`就意味着当`leader` 下线的时候可以从非`ISR`集合中选出新的`leader`，这样有可能造成数据的丢失；如果设置为`false`也会影响可用性，非`ISR`集合中的副本虽然没能及时同步所有的消息，但最起码还是存活的可用副本。设置该值需要在可用性和可靠性之前权衡
- 同步刷盘策略，相关参数(`log.flush.interval.messages`和`log.flush.interval.ms`)，同步刷盘是增强一个组件可靠性的有效方式【绝大多数情景下可靠性不应该由同步刷盘这种极其损耗性能的操作来保障，而应该采用多副本的机制来保障】
- `enable.auto.commit`参数默认为`true`即开启自动位移提交的功能，虽然这种方式非常简便但它会带来重复消费和消息丢失的问题。对于高可靠性要求的应用显然不可取，所以需将`enable.auto.commit`设为`false`来执行手动位移提交。在执行手动位移提交的时候也要遵循一个原则:如果消息没有被成功消费那么就不能提交所对应的消费位移，对于高可靠要求的应用来说宁愿重复消费也不应该因为消费异常而导致消息丢失，有时候由于应用解析消息的异常，可能导致部分消息一直不能够被成功消费，那么这个时候为了不影响整体的消费进度，可以将这类消息暂存到死信队列以便后续的故障排除
- 对于消费端`Kafka`还提供了一个可以兜底的功能，即回溯消费。通过这个功能我们能够有机会对漏掉的消息相应的进行回补，进而可以进一步提高可靠性