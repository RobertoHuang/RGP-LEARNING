# 05-4.部署kube-scheduler集群

本文档讲解部署一个三实例`kube-scheduler`集群的步骤.

注意:如果没有特殊指明，本文档的所有操作**均在roberto-k8s-01节点上执行**

为保证通信安全本文档先生成`x509`证书和私钥，`kube-scheduler`在如下两种情况下使用该证书

- 与`kube-apiserver`的安全端口通信
- 在**安全端口**(`https，10251`) 输出`prometheus`格式的`metrics`

## 创建kube-scheduler证书和私钥

- 创建证书签名请求

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
  cat > kube-scheduler-csr.json <<EOF
  {
      "CN": "system:kube-scheduler",
      "hosts": [
        "127.0.0.1",
        "10.4.7.11",
        "10.4.7.12",
        "10.4.7.13"
      ],
      "key": {
          "algo": "rsa",
          "size": 2048
      },
      "names": [
        {
          "C": "CN",
          "ST": "BeiJing",
          "L": "BeiJing",
          "O": "system:kube-scheduler",
          "OU": "system"
        }
      ]
  }
  EOF
  ```
  
    `hosts`列表包含**所有**`kube-scheduler`节点`IP`
  
    `CN`和`O`均为 `system:kube-scheduler`，`kubernetes`内置的`ClusterRoleBindings`为`system:kube-scheduler` 将赋予`kube-scheduler`工作所需的权限
  
- 生成证书和私钥

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    cfssl gencert -ca=/opt/k8s/work/ca.pem \
      -ca-key=/opt/k8s/work/ca-key.pem \
      -config=/opt/k8s/work/ca-config.json \
      -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
    ```

- 将生成的证书和私钥分发到所有`master`节点

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    for node_ip in ${NODE_IPS[@]}
      do
        echo ">>> ${node_ip}"
        scp kube-scheduler*.pem root@${node_ip}:/etc/kubernetes/cert/
      done
    ```

## 创建和分发kubeconfig文件

- 创建`kubeconfig`模板配置文件

    `kube-scheduler`使用`kubeconfig`文件访问`apiserver`

    该文件提供了`apiserver`地址、嵌入的`CA`证书和`kube-scheduler`证书

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    kubectl config set-cluster kubernetes \
      --certificate-authority=/opt/k8s/work/ca.pem \
      --embed-certs=true \
      --server="https://##NODE_IP##:6443" \
      --kubeconfig=kube-scheduler.kubeconfig
    
    kubectl config set-credentials system:kube-scheduler \
      --client-certificate=kube-scheduler.pem \
      --client-key=kube-scheduler-key.pem \
      --embed-certs=true \
      --kubeconfig=kube-scheduler.kubeconfig
    
    kubectl config set-context system:kube-scheduler \
      --cluster=kubernetes \
      --user=system:kube-scheduler \
      --kubeconfig=kube-scheduler.kubeconfig
    
    kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
    ```

    分发`kubeconfig`到所有`master`节点

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    for node_ip in ${NODE_IPS[@]}
      do
        echo ">>> ${node_ip}"
        sed -e "s/##NODE_IP##/${node_ip}/" kube-scheduler.kubeconfig > kube-scheduler-${node_ip}.kubeconfig
        scp kube-scheduler-${node_ip}.kubeconfig root@${node_ip}:/etc/kubernetes/kube-scheduler.kubeconfig
      done
    ```

## 创建kube-scheduler配置文件

- 创建`kube-scheduler`配置模板文件

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    cat >kube-scheduler.yaml.template <<EOF
    apiVersion: kubescheduler.config.k8s.io/v1alpha1
    kind: KubeSchedulerConfiguration
    bindTimeoutSeconds: 600
    clientConnection:
      burst: 200
      kubeconfig: "/etc/kubernetes/kube-scheduler.kubeconfig"
      qps: 100
    enableContentionProfiling: false
    enableProfiling: true
    hardPodAffinitySymmetricWeight: 1
    healthzBindAddress: ##NODE_IP##:10251
    leaderElection:
      leaderElect: true
    metricsBindAddress: ##NODE_IP##:10251
    EOF
    ```

    `--kubeconfig`:指定`kubeconfig`文件路径，`kube-scheduler`使用它连接和验证`kube-apiserver`

    `--leader-elect=true`:集群运行模式，启用选举功能；被选为`leader`的节点负责处理工作，其它节点为阻塞状态

- 替换模板文件中的变量并分发到所有`master`节点

    替换模板文件中的变量

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    for (( i=0; i < 3; i++ ))
      do
        sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-scheduler.yaml.template > kube-scheduler-${NODE_IPS[i]}.yaml
      done
    ```

    将`kube-scheduler`配置文件发送至所有`master`节点。并重命名为`kube-scheduler.yaml`

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    for node_ip in ${NODE_IPS[@]}
      do
        echo ">>> ${node_ip}"
        scp kube-scheduler-${node_ip}.yaml root@${node_ip}:/etc/kubernetes/kube-scheduler.yaml
      done
    ```

## 创建kube-scheduler systemd unit文件

- 创建`kube-scheduler systemd unit`模板文件

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    cat > kube-scheduler.service.template <<EOF
    [Unit]
    Description=Kubernetes Scheduler
    Documentation=https://github.com/GoogleCloudPlatform/kubernetes
    
    [Service]
    WorkingDirectory=${K8S_DIR}/kube-scheduler
    ExecStart=/opt/k8s/bin/kube-scheduler \\
        --config=/etc/kubernetes/kube-scheduler.yaml \\
        --bind-address=##NODE_IP## \\
        --secure-port=10259 \\
        --port=0 \\
        --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\
        --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\
        --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
        --client-ca-file=/etc/kubernetes/cert/ca.pem \\
        --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
        --requestheader-extra-headers-prefix="X-Remote-Extra-" \\
        --requestheader-group-headers=X-Remote-Group \\
        --requestheader-username-headers=X-Remote-User \\
        --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
        --logtostderr=true \\
        --v=2
    Restart=always
    RestartSec=5
    StartLimitInterval=0
    
    [Install]
    WantedBy=multi-user.target
    EOF
    ```
    
- 替换模板文件中的变量，为各节点创建`systemd unit`文件

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    for (( i=0; i < 3; i++ ))
      do
        sed -e "s/##NODE_NAME##/${NODE_NAMES[i]}/" -e "s/##NODE_IP##/${NODE_IPS[i]}/" kube-scheduler.service.template > kube-scheduler-${NODE_IPS[i]}.service 
      done
    ```

- 分发`systemd unit`文件到所有`master`节点

    ```shell
    cd /opt/k8s/work
    source /opt/k8s/bin/environment.sh
    for node_ip in ${NODE_IPS[@]}
      do
        echo ">>> ${node_ip}"
        scp kube-scheduler-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-scheduler.service
      done
    ```

## 启动kube-scheduler服务

```shell
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "mkdir -p ${K8S_DIR}/kube-scheduler"
    ssh root@${node_ip} "systemctl daemon-reload && systemctl enable kube-scheduler && systemctl restart kube-scheduler"
  done
```

## 检查kube-scheduler服务运行状态

```shell
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo ">>> ${node_ip}"
    ssh root@${node_ip} "systemctl status kube-scheduler|grep Active"
  done
```

确保状态为`active (running)`，否则查看日志确认原因

```shell
journalctl -u kube-scheduler -e
```

## 查看输出的metrics和当前的Leader

- 查看和输出`metric`

    注意:以下命令在`kube-scheduler`节点上执行

    `kube-scheduler`监听`10251`和`10259`端口:

    - `10251`:接收http`请求`，非安全端口不需要认证授权
    - `10259`:接收`https`请求，安全端口需要认证授权

    两个接口都对外提供`/metrics`和`/healthz`的访问

    ```shell
    # sudo netstat -lnpt |grep kube-sch
    tcp        0      0 10.4.7.11:10251         0.0.0.0:*               LISTEN      10176/kube-schedule
    tcp        0      0 10.4.7.11:10259         0.0.0.0:*               LISTEN      10176/kube-schedule
    ```

    ```shell
    # curl -s http://10.4.7.11:10251/metrics |head
    
    # HELP apiserver_audit_event_total [ALPHA] Counter of audit events generated and sent to the audit backend.
    # TYPE apiserver_audit_event_total counter
    apiserver_audit_event_total 0
    # HELP apiserver_audit_requests_rejected_total [ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.
    # TYPE apiserver_audit_requests_rejected_total counter
    apiserver_audit_requests_rejected_total 0
    # HELP apiserver_client_certificate_expiration_seconds [ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.
    # TYPE apiserver_client_certificate_expiration_seconds histogram
    apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
    ```

    ```shell
    # curl -s --cacert /opt/k8s/work/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://10.4.7.11:10259/metrics | head
    
    # HELP apiserver_audit_event_total [ALPHA] Counter of audit events generated and sent to the audit backend.
    # TYPE apiserver_audit_event_total counter
    apiserver_audit_event_total 0
    # HELP apiserver_audit_requests_rejected_total [ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.
    # TYPE apiserver_audit_requests_rejected_total counter
    apiserver_audit_requests_rejected_total 0
    # HELP apiserver_client_certificate_expiration_seconds [ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.
    # TYPE apiserver_client_certificate_expiration_seconds histogram
    apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
    ```

- 查看当前的`Leader`

    ```shell
    # kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
    apiVersion: v1
    kind: Endpoints
    metadata:
      annotations:
        control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"roberto-k8s-01_b7026166-c645-4beb-9556-eb47bc206c28","leaseDurationSeconds":15,"acquireTime":"2020-08-12T16:35:23Z","renewTime":"2020-08-12T16:39:40Z","leaderTransitions":4}'
      creationTimestamp: "2020-08-12T15:48:08Z"
      managedFields:
      - apiVersion: v1
        fieldsType: FieldsV1
        fieldsV1:
          f:metadata:
            f:annotations:
              .: {}
              f:control-plane.alpha.kubernetes.io/leader: {}
        manager: kube-scheduler
        operation: Update
        time: "2020-08-12T16:39:40Z"
      name: kube-scheduler
      namespace: kube-system
      resourceVersion: "11414"
      selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
      uid: 6d98aa0d-1ad3-412a-b04e-ba1c3b49cb4d
    ```

    可见当前的`leader`为`roberto-k8s-01`节点

## 测试kube-scheduler集群的高可用

停掉一个或两个节点的`kube-scheduler`服务，观察其它节点的日志，看是否获取了`leader`权限

